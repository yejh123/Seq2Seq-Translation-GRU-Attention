dataset size: 6834
dataset size: 68
Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
Seq2Seq(
  (init_affine): Linear(in_features=2048, out_features=1024, bias=True)
  (encoder): Encoder(
    (embedding): Embedding(13290, 512)
    (rnn): GRU(512, 1024, batch_first=True, bidirectional=True)
    (enc_emb_dp): Dropout(p=0.4, inplace=False)
    (enc_hid_dp): Dropout(p=0.4, inplace=False)
  )
  (decoder): Decoder(
    (embedding): Embedding(11873, 512)
    (dec_emb_dp): Dropout(p=0.4, inplace=False)
    (attention): Attention(
      (h2s): Linear(in_features=1024, out_features=1000, bias=True)
      (s2s): Linear(in_features=2048, out_features=1000, bias=True)
      (a2o): Linear(in_features=1000, out_features=1, bias=True)
    )
    (gru1): GRUCell(512, 1024)
    (gru2): GRUCell(2048, 1024)
    (embedding2out): Linear(in_features=512, out_features=620, bias=True)
    (hidden2out): Linear(in_features=1024, out_features=620, bias=True)
    (c2o): Linear(in_features=2048, out_features=620, bias=True)
    (readout_dp): Dropout(p=0.4, inplace=False)
    (affine): Linear(in_features=620, out_features=11873, bias=True)
  )
)
train[10] loss: 231.632, Perplexity: 39474820495189615346313528736204162045762618638327697855453131868119545369583879197072613175868784640.000
train[20] loss: 189.447, Perplexity: 18870271611949007401322190638922386362262174816799021477690372195710153920987791360.000
train[30] loss: 185.011, Perplexity: 223580889198031362202490611444433188100167624561129394013938657882573221388091392.000
train[40] loss: 182.441, Perplexity: 17105938325316674967924191130617351681169872343417531212831106231723416577114112.000
train[50] loss: 179.312, Perplexity: 748225216311955606966266804217976732927249048533462668559594060283415497801728.000
train[60] loss: 175.010, Perplexity: 10132111129518467117359946043382361910118139025474955465278845637225147793408.000
train[70] loss: 172.069, Perplexity: 535585373029833793311399348717606965260818398341660239977456866854162137088.000
train[80] loss: 168.961, Perplexity: 23915833374069378105430113713741902620956330220514662863273261821918707712.000
train[90] loss: 166.756, Perplexity: 2638752875492463455942133326334025690135360424213796860426447037564715008.000
train[100] loss: 163.124, Perplexity: 69810310978801714858034618811264010613526890254440045472106031274262528.000
train[110] loss: 162.898, Perplexity: 55657632525541862784717450648831917684299250331369869578455371252498432.000
train[120] loss: 157.005, Perplexity: 153540761153207959826950203572236982434291888642985817573524523450368.000
train[130] loss: 155.589, Perplexity: 37274655218182170280710558882916041459738571397766184655424004816896.000
train[140] loss: 155.373, Perplexity: 30033402921043809730597382298019789066907122078898738140732257730560.000
train[150] loss: 154.321, Perplexity: 10492548048130336453387997218947540793885653533310982910484804534272.000
train[160] loss: 154.399, Perplexity: 11342847324553829080540091527815251241683612223482808213656265818112.000
train[170] loss: 151.552, Perplexity: 658128692766809765281111244887070177989427253152369606320589897728.000
train[180] loss: 151.353, Perplexity: 539187148731691216632386690930912009708662075803937862070399139840.000
train[190] loss: 149.919, Perplexity: 128576088209573788846267628232610104333970158367571973885044719616.000
train[200] loss: 149.829, Perplexity: 117461903552394427343770795287887266142402628077727916436846804992.000
D:\Program Files\Anaconda3\lib\site-packages\nltk\translate\bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 2-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
D:\Program Files\Anaconda3\lib\site-packages\nltk\translate\bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 3-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
D:\Program Files\Anaconda3\lib\site-packages\nltk\translate\bleu_score.py:516: UserWarning: 
The hypothesis contains 0 counts of 4-gram overlaps.
Therefore the BLEU score evaluates to 0, independently of
how many N-gram overlaps of lower order it contains.
Consider using lower n-gram order or use SmoothingFunction()
  warnings.warn(_msg)
 test_epoch[68] val[200] loss: 8.768, Perplexity: 6427.726, blue score: 0.237
train[210] loss: 148.026, Perplexity: 19361412224645284606723031146170046697921869986824146951564427264.000
train[220] loss: 144.663, Perplexity: 670149774800275280305436469842351870118427801604952330629808128.000
train[230] loss: 139.448, Perplexity: 3643537419444361745632426588886229777950793783513272907988992.000
train[240] loss: 139.724, Perplexity: 4802145591904735817000227616241974709355585377730016787300352.000
train[250] loss: 139.312, Perplexity: 3179705703472138614289313089705775171272172133542618663485440.000
train[260] loss: 138.875, Perplexity: 2055037715108596714154845281607523550988621090738700906659840.000
train[270] loss: 138.288, Perplexity: 1141626419731323026879252591697963941207495372673885147234304.000
train[280] loss: 138.062, Perplexity: 911109369115866042533974563527875018921954753039202613460992.000
train[290] loss: 138.175, Perplexity: 1020457928462248242838290689174190316660916911000645575639040.000
train[300] loss: 136.753, Perplexity: 246173266810645635614455587288297957062920689676985741344768.000
train[310] loss: 137.404, Perplexity: 471811167897391310788210041364127473292994946337200712712192.000
train[320] loss: 135.189, Perplexity: 51479372501341936021791373322010089599385971132310029860864.000
train[330] loss: 128.149, Perplexity: 45133177758814364810851307237662732575460256703789924352.000
train[340] loss: 125.387, Perplexity: 2849295721384094208361860449007857237517450978795716608.000
train[350] loss: 126.504, Perplexity: 8706894237023382065333354845690382585496263130351665152.000
train[360] loss: 128.756, Perplexity: 82767310496273699410587936179056636521803965864951676928.000
train[370] loss: 127.783, Perplexity: 31296893374381202206558304135272435309768298577249959936.000
train[380] loss: 126.052, Perplexity: 5540251836453360174707390436016778763380431318870392832.000
train[390] loss: 126.472, Perplexity: 8437137881261294805227285136553743357135354877066608640.000
train[400] loss: 125.610, Perplexity: 3563302830981016110349383008719007823684282992539205632.000
 test_epoch[68] val[400] loss: 8.767, Perplexity: 6418.919, blue score: 0.331
train[410] loss: 127.462, Perplexity: 22709594165213649565312762552398273043369460961281835008.000
train[420] loss: 125.458, Perplexity: 3060381063512822908803712441271505354925252389019582464.000
train[430] loss: 123.481, Perplexity: 423623428032929504974336108624487846957393668231659520.000
train[440] loss: 116.774, Perplexity: 518206124230859110327343864601243847103488607649792.000
train[450] loss: 117.528, Perplexity: 1100645604832468610447227263768052566891334324977664.000
train[460] loss: 115.690, Perplexity: 175254495095215600474567150022531338562848715440128.000
train[470] loss: 116.607, Perplexity: 438448696300788386862883112477686102575378830721024.000
train[480] loss: 115.334, Perplexity: 122765969397017098932598558555336860293952914325504.000
train[490] loss: 116.663, Perplexity: 463711056938645642291791686853329127535850066804736.000
train[500] loss: 115.629, Perplexity: 164824629721588021534559600942237328254735740829696.000
train[510] loss: 116.864, Perplexity: 566804619191145391823996733974692211937755984297984.000
train[520] loss: 117.983, Perplexity: 1736034880050683677561971154463344687682170386382848.000
train[530] loss: 114.340, Perplexity: 45404329381858396288261594340140852928951131045888.000
train[540] loss: 111.706, Perplexity: 3262068853477920812054283847143735969296454516736.000
train[550] loss: 107.704, Perplexity: 59607962398360396137800667116799824971081711616.000
train[560] loss: 105.904, Perplexity: 9849672543212393176602276849994684590744141824.000
train[570] loss: 107.654, Perplexity: 56677751999329062585437765577820222969647988736.000
train[580] loss: 106.870, Perplexity: 25884071654459776520730605613278744325413404672.000
train[590] loss: 106.007, Perplexity: 10924933749455631431030652775903915450042417152.000
train[600] loss: 107.009, Perplexity: 29757877309446792537676147988735678648894357504.000
 test_epoch[68] val[600] loss: 8.569, Perplexity: 5268.370, blue score: 0.368
train[610] loss: 107.575, Perplexity: 52384889029868241599849298919276016352060506112.000
train[620] loss: 107.164, Perplexity: 34745946061740635670310169759602697083360903168.000
train[630] loss: 107.936, Perplexity: 75192047916612368738623568739751282114222358528.000
train[640] loss: 108.002, Perplexity: 80296756343783396024365997487652966675085524992.000
train[650] loss: 100.918, Perplexity: 67334282317174827955225086633308022571532288.000
train[660] loss: 98.559, Perplexity: 6362765513910095472704715764285731613179904.000
train[670] loss: 98.952, Perplexity: 9423013247110002309469352687919956857192448.000
train[680] loss: 98.345, Perplexity: 5134986044956549072333127607477887532269568.000
train[690] loss: 98.943, Perplexity: 9344327246025434162246781984150012892807168.000
train[700] loss: 99.613, Perplexity: 18256193683236781559572462565380179682656256.000
train[710] loss: 98.276, Perplexity: 4795511454186009024438188236661891065184256.000
train[720] loss: 99.875, Perplexity: 23729193729561319879155354955428959411503104.000
train[730] loss: 98.343, Perplexity: 5126311793173992888937664052147402416586752.000
train[740] loss: 99.603, Perplexity: 18072282896014885980425468840335200156123136.000
train[750] loss: 97.898, Perplexity: 3286662951345431947734570374229629748641792.000
train[760] loss: 90.897, Perplexity: 2992214200490799508711601700904013660160.000
train[770] loss: 90.443, Perplexity: 1900328045966242979648513794478104903680.000
train[780] loss: 91.235, Perplexity: 4196787336405388884305662740236205031424.000
train[790] loss: 91.336, Perplexity: 4641073447735289088583619709804537184256.000
train[800] loss: 90.444, Perplexity: 1902630327995283421329685034881194655744.000
 test_epoch[68] val[800] loss: 8.717, Perplexity: 6104.033, blue score: 0.407
train[810] loss: 90.191, Perplexity: 1476901026005763188236857372118966337536.000
train[820] loss: 91.623, Perplexity: 6184073774737690611060356140848866394112.000
train[830] loss: 92.025, Perplexity: 9243731304203906763835102091392959119360.000
train[840] loss: 91.024, Perplexity: 3396598578836113950739856980053485158400.000
train[850] loss: 92.204, Perplexity: 11060816530744292096419739328777570746368.000
train[860] loss: 89.492, Perplexity: 734141275668997797169893369665577549824.000
train[870] loss: 83.177, Perplexity: 1327702923823845155918820495008464896.000
train[880] loss: 84.061, Perplexity: 3216156320931132395597929904045293568.000
train[890] loss: 82.269, Perplexity: 535805251396323908750301600721928192.000
train[900] loss: 83.414, Perplexity: 1684044396873285601804359101237428224.000
train[910] loss: 83.616, Perplexity: 2060381669421439273549263286046818304.000
train[920] loss: 83.817, Perplexity: 2518230633272838731863969179042316288.000
train[930] loss: 83.633, Perplexity: 2096588371355593935683791544630902784.000
train[940] loss: 85.177, Perplexity: 9812657190651429228591894004280328192.000
train[950] loss: 85.098, Perplexity: 9067064812331069034401743551678382080.000
train[960] loss: 86.567, Perplexity: 39388268741267139758356481087440420864.000
train[970] loss: 79.690, Perplexity: 40628108318998961815205554113478656.000
train[980] loss: 75.627, Perplexity: 698696507355570774629798037684224.000
train[990] loss: 76.008, Perplexity: 1022516140445619570497839173730304.000
train[1000] loss: 78.271, Perplexity: 9830971170531007897766651274199040.000
 test_epoch[68] val[1000] loss: 8.805, Perplexity: 6666.112, blue score: 0.445
train[1010] loss: 76.356, Perplexity: 1448982842804651202098512181329920.000
train[1020] loss: 77.975, Perplexity: 7310658484425624624460865237680128.000
train[1030] loss: 77.448, Perplexity: 4319666755376744911953564494987264.000
train[1040] loss: 78.535, Perplexity: 12808677096233001252472285837656064.000
train[1050] loss: 79.148, Perplexity: 23625089725666007229106747074412544.000
train[1060] loss: 78.501, Perplexity: 12373292509053236911965393712578560.000
train[1070] loss: 78.706, Perplexity: 15186155986309088885608794606796800.000
train[1080] loss: 70.185, Perplexity: 3027231403521175186610908561408.000
train[1090] loss: 68.962, Perplexity: 891196014515033559127893737472.000
train[1100] loss: 70.869, Perplexity: 5997579082675736068825481740288.000
train[1110] loss: 71.226, Perplexity: 8573404070865452409680771940352.000
train[1120] loss: 70.706, Perplexity: 5098334332854338910332089532416.000
train[1130] loss: 72.314, Perplexity: 25440078948036321084926041522176.000
train[1140] loss: 72.487, Perplexity: 30248273017085334997384673886208.000
train[1150] loss: 71.921, Perplexity: 17167164356591838183630720466944.000
train[1160] loss: 71.936, Perplexity: 17437840455378796925811318325248.000
train[1170] loss: 73.851, Perplexity: 118321225199395263303115058708480.000
train[1180] loss: 69.770, Perplexity: 1998408584569902700394180509696.000
train[1190] loss: 63.757, Perplexity: 4887776257787136584697511936.000
train[1200] loss: 65.461, Perplexity: 26862186987978733858326577152.000
 test_epoch[68] val[1200] loss: 8.483, Perplexity: 4831.018, blue score: 0.506
train[1210] loss: 64.828, Perplexity: 14273748681380133278268260352.000
train[1220] loss: 64.594, Perplexity: 11296436361233824658195742720.000
train[1230] loss: 65.808, Perplexity: 38035997922178600080343302144.000
train[1240] loss: 65.268, Perplexity: 22162761542021351231593644032.000
train[1250] loss: 66.244, Perplexity: 58801381484808259286358556672.000
train[1260] loss: 66.583, Perplexity: 82548922559892366809949536256.000
train[1270] loss: 65.750, Perplexity: 35891880434390498005099741184.000
train[1280] loss: 67.876, Perplexity: 300622113591091640086139240448.000
train[1290] loss: 62.573, Perplexity: 1496640669288953245583015936.000
train[1300] loss: 59.632, Perplexity: 79022775130848869193089024.000
train[1310] loss: 58.591, Perplexity: 27906949306349475713777664.000
train[1320] loss: 58.811, Perplexity: 34762518034708379140620288.000
train[1330] loss: 59.535, Perplexity: 71745030848867892801306624.000
train[1340] loss: 60.370, Perplexity: 165354514262637333012545536.000
train[1350] loss: 60.129, Perplexity: 129938496932830320061841408.000
train[1360] loss: 62.665, Perplexity: 1640401376264132521615163392.000
train[1370] loss: 61.713, Perplexity: 633333033709984902090326016.000
train[1380] loss: 61.195, Perplexity: 377444098607144452677959680.000
train[1390] loss: 61.888, Perplexity: 754481482884041379252535296.000
train[1400] loss: 55.115, Perplexity: 863216489049795835461632.000
 test_epoch[68] val[1400] loss: 8.483, Perplexity: 4833.168, blue score: 0.563
train[1410] loss: 53.769, Perplexity: 224679288264226529869824.000
train[1420] loss: 54.674, Perplexity: 555374278205410425962496.000
train[1430] loss: 54.893, Perplexity: 691601985047139490725888.000
train[1440] loss: 55.220, Perplexity: 958635158515864456658944.000
train[1450] loss: 54.648, Perplexity: 541233455649376994066432.000
train[1460] loss: 55.965, Perplexity: 2019611725641598829068288.000
train[1470] loss: 55.694, Perplexity: 1539989022701931567513600.000
train[1480] loss: 57.445, Perplexity: 8875503478064245110734848.000
train[1490] loss: 56.209, Perplexity: 2578510333954327930994688.000
train[1500] loss: 55.632, Perplexity: 1447358134504858908098560.000
train[1510] loss: 48.876, Perplexity: 1684751836280763187200.000
train[1520] loss: 49.032, Perplexity: 1968649094339405283328.000
train[1530] loss: 50.305, Perplexity: 7030704183433447342080.000
train[1540] loss: 50.147, Perplexity: 6003780183332268015616.000
train[1550] loss: 50.339, Perplexity: 7276550445739634851840.000
train[1560] loss: 51.882, Perplexity: 34058694022473529163776.000
train[1570] loss: 51.879, Perplexity: 33938480484520197881856.000
train[1580] loss: 51.824, Perplexity: 32118667795409026416640.000
train[1590] loss: 50.988, Perplexity: 13919257588741686951936.000
train[1600] loss: 52.219, Perplexity: 47694309444722580520960.000
 test_epoch[68] val[1600] loss: 8.192, Perplexity: 3611.771, blue score: 0.615
train[1610] loss: 48.157, Perplexity: 820910217037915357184.000
train[1620] loss: 45.280, Perplexity: 46225640140827009024.000
train[1630] loss: 44.999, Perplexity: 34892917184395730944.000
train[1640] loss: 45.477, Perplexity: 56290188700744310784.000
train[1650] loss: 46.433, Perplexity: 146376102682455408640.000
train[1660] loss: 47.544, Perplexity: 444947067484829057024.000
train[1670] loss: 47.158, Perplexity: 302300855830664380416.000
train[1680] loss: 47.402, Perplexity: 385690418197058945024.000
train[1690] loss: 47.937, Perplexity: 658963475890968461312.000
train[1700] loss: 48.135, Perplexity: 802917122445759217664.000
train[1710] loss: 47.699, Perplexity: 519523352692401897472.000
train[1720] loss: 42.908, Perplexity: 4310897707773877248.000
train[1730] loss: 41.197, Perplexity: 779347026596641536.000
train[1740] loss: 42.354, Perplexity: 2478042165815161344.000
train[1750] loss: 42.855, Perplexity: 4090435138725292544.000
train[1760] loss: 42.984, Perplexity: 4654109668536816640.000
train[1770] loss: 43.514, Perplexity: 7905400115651831808.000
train[1780] loss: 43.219, Perplexity: 5883090043136650240.000
train[1790] loss: 43.830, Perplexity: 10845532105844713472.000
train[1800] loss: 42.730, Perplexity: 3608948190541476352.000
 test_epoch[68] val[1800] loss: 8.444, Perplexity: 4645.728, blue score: 0.663
train[1810] loss: 43.809, Perplexity: 10621030852382015488.000
train[1820] loss: 43.553, Perplexity: 8221450693881207808.000
train[1830] loss: 38.841, Perplexity: 73842611874953968.000
train[1840] loss: 37.663, Perplexity: 22744819151980292.000
train[1850] loss: 38.596, Perplexity: 57818002490157816.000
train[1860] loss: 39.547, Perplexity: 149695508784735776.000
train[1870] loss: 38.851, Perplexity: 74613956822032752.000
train[1880] loss: 39.026, Perplexity: 88874473417019248.000
train[1890] loss: 39.000, Perplexity: 86573318834325776.000
train[1900] loss: 40.109, Perplexity: 262387472568866912.000
train[1910] loss: 40.527, Perplexity: 398894229133717632.000
train[1920] loss: 40.089, Perplexity: 257243248131302784.000
train[1930] loss: 39.121, Perplexity: 97752265360850256.000
train[1940] loss: 33.809, Perplexity: 481803886795580.438
train[1950] loss: 35.510, Perplexity: 2640417289143117.000
train[1960] loss: 35.100, Perplexity: 1752112000097494.250
train[1970] loss: 35.315, Perplexity: 2172867054253839.750
train[1980] loss: 36.367, Perplexity: 6221404496499415.000
train[1990] loss: 36.552, Perplexity: 7487106831916624.000
train[2000] loss: 36.275, Perplexity: 5674948454708820.000
 test_epoch[68] val[2000] loss: 7.608, Perplexity: 2014.239, blue score: 0.734
train[2010] loss: 36.974, Perplexity: 11414715295003030.000
train[2020] loss: 37.170, Perplexity: 13887032737922306.000
train[2030] loss: 37.541, Perplexity: 20125288535854536.000
train[2040] loss: 33.824, Perplexity: 489355631243255.562
train[2050] loss: 31.688, Perplexity: 57786934161073.602
train[2060] loss: 32.243, Perplexity: 100726548366476.203
train[2070] loss: 31.835, Perplexity: 66948061958483.203
train[2080] loss: 32.972, Perplexity: 208806615942289.812
train[2090] loss: 33.085, Perplexity: 233794071441275.156
train[2100] loss: 32.799, Perplexity: 175552850848340.219
train[2110] loss: 34.243, Perplexity: 743997206005580.375
train[2120] loss: 34.197, Perplexity: 710197689868776.250
train[2130] loss: 34.698, Perplexity: 1172726211651832.250
train[2140] loss: 34.686, Perplexity: 1158493307293324.750
train[2150] loss: 28.665, Perplexity: 2813619957972.187
train[2160] loss: 29.287, Perplexity: 5240656834718.623
train[2170] loss: 29.670, Perplexity: 7683584371788.174
train[2180] loss: 29.930, Perplexity: 9963452871874.861
train[2190] loss: 29.695, Perplexity: 7880629073438.583
train[2200] loss: 30.497, Perplexity: 17558259833257.760
 test_epoch[68] val[2200] loss: 6.774, Perplexity: 874.387, blue score: 0.818
train[2210] loss: 30.635, Perplexity: 20157154865073.938
train[2220] loss: 32.105, Perplexity: 87730338195861.359
train[2230] loss: 30.830, Perplexity: 24503223169766.086
train[2240] loss: 31.668, Perplexity: 56633273410384.727
train[2250] loss: 30.105, Perplexity: 11872457619377.506
train[2260] loss: 26.825, Perplexity: 446596790336.835
train[2270] loss: 26.713, Perplexity: 399340248367.333
train[2280] loss: 26.608, Perplexity: 359528234033.520
train[2290] loss: 27.607, Perplexity: 975870570435.615
train[2300] loss: 27.519, Perplexity: 894114784768.893
train[2310] loss: 28.049, Perplexity: 1519398822754.934
train[2320] loss: 28.742, Perplexity: 3036365734398.106
train[2330] loss: 28.708, Perplexity: 2936082887987.667
train[2340] loss: 28.442, Perplexity: 2249996652487.308
train[2350] loss: 29.212, Perplexity: 4860857715544.113
train[2360] loss: 25.701, Perplexity: 145115213322.989
train[2370] loss: 24.252, Perplexity: 34077921569.317
train[2380] loss: 24.891, Perplexity: 64568686886.819
train[2390] loss: 24.641, Perplexity: 50270857667.885
train[2400] loss: 24.952, Perplexity: 68649197281.869
 test_epoch[68] val[2400] loss: 6.201, Perplexity: 493.249, blue score: 0.866
train[2410] loss: 25.430, Perplexity: 110709004285.450
train[2420] loss: 25.912, Perplexity: 179176349858.052
train[2430] loss: 26.099, Perplexity: 216180127965.953
train[2440] loss: 26.416, Perplexity: 296687559726.868
train[2450] loss: 26.971, Perplexity: 516795423205.774
train[2460] loss: 26.966, Perplexity: 514349377552.986
train[2470] loss: 22.468, Perplexity: 5726817199.221
train[2480] loss: 22.584, Perplexity: 6427153878.575
train[2490] loss: 22.462, Perplexity: 5687359931.025
train[2500] loss: 22.858, Perplexity: 8456819406.142
train[2510] loss: 23.285, Perplexity: 12952342091.294
train[2520] loss: 23.755, Perplexity: 20735847736.768
train[2530] loss: 24.061, Perplexity: 28145032514.761
train[2540] loss: 24.232, Perplexity: 33422039489.779
train[2550] loss: 24.261, Perplexity: 34381607147.728
train[2560] loss: 24.591, Perplexity: 47810090295.646
train[2570] loss: 23.701, Perplexity: 19646356940.042
train[2580] loss: 20.397, Perplexity: 721402314.512
train[2590] loss: 21.092, Perplexity: 1445200386.194
train[2600] loss: 20.834, Perplexity: 1117386878.163
 test_epoch[68] val[2600] loss: 5.355, Perplexity: 211.657, blue score: 0.903
train[2610] loss: 21.058, Perplexity: 1396998466.852
train[2620] loss: 21.758, Perplexity: 2813914595.650
train[2630] loss: 21.560, Perplexity: 2307879808.121
train[2640] loss: 22.052, Perplexity: 3775481878.328
train[2650] loss: 22.150, Perplexity: 4163490719.746
train[2660] loss: 22.512, Perplexity: 5984165282.999
train[2670] loss: 22.870, Perplexity: 8559483046.778
train[2680] loss: 20.639, Perplexity: 918830104.735
train[2690] loss: 19.010, Perplexity: 180345177.356
train[2700] loss: 19.311, Perplexity: 243646123.496
train[2710] loss: 18.959, Perplexity: 171353461.263
train[2720] loss: 19.436, Perplexity: 275970664.504
train[2730] loss: 19.499, Perplexity: 293838394.859
train[2740] loss: 20.284, Perplexity: 644754971.077
train[2750] loss: 20.124, Perplexity: 549455362.238
train[2760] loss: 20.556, Perplexity: 845902521.493
train[2770] loss: 21.036, Perplexity: 1366519661.374
train[2780] loss: 20.565, Perplexity: 853705834.763
train[2790] loss: 17.980, Perplexity: 64343804.258
train[2800] loss: 17.212, Perplexity: 29855406.024
 test_epoch[68] val[2800] loss: 4.501, Perplexity: 90.088, blue score: 0.959
train[2810] loss: 17.249, Perplexity: 30977053.138
train[2820] loss: 18.236, Perplexity: 83159840.312
train[2830] loss: 18.135, Perplexity: 75153621.316
train[2840] loss: 18.051, Perplexity: 69111333.153
train[2850] loss: 18.500, Perplexity: 108212894.663
train[2860] loss: 18.701, Perplexity: 132384999.642
train[2870] loss: 19.015, Perplexity: 181111159.912
train[2880] loss: 19.691, Perplexity: 356288914.655
train[2890] loss: 19.191, Perplexity: 215970497.453
train[2900] loss: 15.457, Perplexity: 5162385.181
train[2910] loss: 16.017, Perplexity: 9039369.214
train[2920] loss: 16.249, Perplexity: 11396578.516
train[2930] loss: 16.461, Perplexity: 14091533.969
train[2940] loss: 16.595, Perplexity: 16110355.166
train[2950] loss: 16.917, Perplexity: 22232136.895
train[2960] loss: 17.281, Perplexity: 32001602.553
train[2970] loss: 17.847, Perplexity: 56342950.982
train[2980] loss: 17.684, Perplexity: 47855679.889
train[2990] loss: 17.742, Perplexity: 50742165.997
train[3000] loss: 16.785, Perplexity: 19482357.581
 test_epoch[68] val[3000] loss: 4.055, Perplexity: 57.680, blue score: 0.971
train[3010] loss: 14.485, Perplexity: 1952579.477
train[3020] loss: 14.917, Perplexity: 3008214.127
train[3030] loss: 15.130, Perplexity: 3721307.517
train[3040] loss: 15.138, Perplexity: 3753736.029
train[3050] loss: 16.053, Perplexity: 9368120.684
train[3060] loss: 16.014, Perplexity: 9012785.338
train[3070] loss: 15.879, Perplexity: 7874162.331
train[3080] loss: 16.227, Perplexity: 11154729.900
train[3090] loss: 16.348, Perplexity: 12579164.535
train[3100] loss: 16.481, Perplexity: 14369315.203
train[3110] loss: 14.509, Perplexity: 2001194.172
train[3120] loss: 13.463, Perplexity: 702635.240
train[3130] loss: 13.597, Perplexity: 803924.752
train[3140] loss: 14.291, Perplexity: 1609269.723
train[3150] loss: 14.139, Perplexity: 1382262.696
train[3160] loss: 13.911, Perplexity: 1100219.622
train[3170] loss: 15.112, Perplexity: 3656004.865
train[3180] loss: 15.064, Perplexity: 3483835.532
train[3190] loss: 14.950, Perplexity: 3108990.092
train[3200] loss: 15.380, Perplexity: 4780990.180
 test_epoch[68] val[3200] loss: 3.663, Perplexity: 38.982, blue score: 0.989
train[3210] loss: 15.483, Perplexity: 5298089.521
train[3220] loss: 12.300, Perplexity: 219719.959
train[3230] loss: 12.387, Perplexity: 239618.725
train[3240] loss: 13.126, Perplexity: 501997.540
train[3250] loss: 13.084, Perplexity: 481315.472
train[3260] loss: 13.560, Perplexity: 774556.149
train[3270] loss: 13.349, Perplexity: 627148.634
train[3280] loss: 13.807, Perplexity: 991892.671
train[3290] loss: 13.779, Perplexity: 964475.854
train[3300] loss: 13.965, Perplexity: 1161564.696
train[3310] loss: 14.306, Perplexity: 1633774.191
train[3320] loss: 13.566, Perplexity: 779498.810
train[3330] loss: 11.493, Perplexity: 98075.221
train[3340] loss: 11.819, Perplexity: 135783.197
train[3350] loss: 11.717, Perplexity: 122652.279
train[3360] loss: 12.290, Perplexity: 217462.223
train[3370] loss: 12.569, Perplexity: 287477.759
train[3380] loss: 12.865, Perplexity: 386687.364
train[3390] loss: 12.867, Perplexity: 387248.528
train[3400] loss: 12.925, Perplexity: 410255.560
 test_epoch[68] val[3400] loss: 3.665, Perplexity: 39.072, blue score: 0.985
train[3410] loss: 13.533, Perplexity: 753515.339
train[3420] loss: 13.104, Perplexity: 490790.924
train[3430] loss: 11.936, Perplexity: 152733.043
train[3440] loss: 11.265, Perplexity: 78046.683
train[3450] loss: 11.240, Perplexity: 76146.996
train[3460] loss: 11.470, Perplexity: 95807.450
train[3470] loss: 11.901, Perplexity: 147435.621
train[3480] loss: 11.566, Perplexity: 105424.126
train[3490] loss: 11.918, Perplexity: 149895.793
train[3500] loss: 12.126, Perplexity: 184607.380
train[3510] loss: 12.501, Perplexity: 268623.030
train[3520] loss: 12.285, Perplexity: 216451.694
train[3530] loss: 12.274, Perplexity: 214158.335
train[3540] loss: 10.499, Perplexity: 36287.935
train[3550] loss: 10.352, Perplexity: 31310.906
train[3560] loss: 10.881, Perplexity: 53176.415
train[3570] loss: 10.989, Perplexity: 59238.252
train[3580] loss: 10.715, Perplexity: 45014.043
train[3590] loss: 11.187, Perplexity: 72219.535
train[3600] loss: 11.285, Perplexity: 79647.260
 test_epoch[68] val[3600] loss: 3.679, Perplexity: 39.609, blue score: 0.985
train[3610] loss: 11.444, Perplexity: 93302.847
train[3620] loss: 11.572, Perplexity: 106114.737
train[3630] loss: 11.410, Perplexity: 90239.475
train[3640] loss: 11.405, Perplexity: 89790.737
train[3650] loss: 9.672, Perplexity: 15862.985
train[3660] loss: 9.755, Perplexity: 17233.564
train[3670] loss: 10.000, Perplexity: 22021.887
train[3680] loss: 10.031, Perplexity: 22720.922
train[3690] loss: 10.642, Perplexity: 41837.807
train[3700] loss: 10.499, Perplexity: 36293.362
train[3710] loss: 10.854, Perplexity: 51762.530
train[3720] loss: 10.820, Perplexity: 50024.218
train[3730] loss: 10.622, Perplexity: 41018.571
train[3740] loss: 11.069, Perplexity: 64132.480
train[3750] loss: 10.012, Perplexity: 22285.995
train[3760] loss: 8.880, Perplexity: 7185.257
train[3770] loss: 9.370, Perplexity: 11725.421
train[3780] loss: 9.447, Perplexity: 12666.145
train[3790] loss: 9.649, Perplexity: 15500.463
train[3800] loss: 9.868, Perplexity: 19312.027
 test_epoch[68] val[3800] loss: 3.658, Perplexity: 38.783, blue score: 0.993
train[3810] loss: 9.987, Perplexity: 21747.879
train[3820] loss: 10.424, Perplexity: 33659.314
train[3830] loss: 10.180, Perplexity: 26382.615
train[3840] loss: 10.309, Perplexity: 30013.609
train[3850] loss: 10.684, Perplexity: 43648.488
train[3860] loss: 9.181, Perplexity: 9706.631
train[3870] loss: 8.508, Perplexity: 4955.813
train[3880] loss: 8.947, Perplexity: 7686.602
train[3890] loss: 9.097, Perplexity: 8929.221
train[3900] loss: 9.241, Perplexity: 10314.426
train[3910] loss: 9.356, Perplexity: 11565.736
train[3920] loss: 9.555, Perplexity: 14116.226
train[3930] loss: 9.759, Perplexity: 17300.884
train[3940] loss: 10.028, Perplexity: 22651.401
train[3950] loss: 9.933, Perplexity: 20594.566
train[3960] loss: 9.681, Perplexity: 16016.494
train[3970] loss: 8.113, Perplexity: 3338.985
train[3980] loss: 8.202, Perplexity: 3647.098
train[3990] loss: 8.512, Perplexity: 4972.088
train[4000] loss: 8.634, Perplexity: 5617.323
 test_epoch[68] val[4000] loss: 3.756, Perplexity: 42.789, blue score: 0.990
train[4010] loss: 8.688, Perplexity: 5931.121
train[4020] loss: 9.010, Perplexity: 8187.512
train[4030] loss: 9.246, Perplexity: 10361.803
train[4040] loss: 9.435, Perplexity: 12521.634
train[4050] loss: 9.515, Perplexity: 13557.924
train[4060] loss: 9.177, Perplexity: 9673.428
train[4070] loss: 8.916, Perplexity: 7448.410
train[4080] loss: 7.740, Perplexity: 2299.619
train[4090] loss: 7.923, Perplexity: 2758.910
train[4100] loss: 8.010, Perplexity: 3011.791
train[4110] loss: 8.531, Perplexity: 5069.029
train[4120] loss: 8.128, Perplexity: 3388.215
train[4130] loss: 8.230, Perplexity: 3750.520
train[4140] loss: 8.763, Perplexity: 6391.730
train[4150] loss: 8.966, Perplexity: 7830.102
train[4160] loss: 8.970, Perplexity: 7867.518
train[4170] loss: 8.750, Perplexity: 6308.351
train[4180] loss: 8.213, Perplexity: 3690.434
train[4190] loss: 7.380, Perplexity: 1603.848
train[4200] loss: 7.786, Perplexity: 2406.051
 test_epoch[68] val[4200] loss: 3.504, Perplexity: 33.238, blue score: 0.998
train[4210] loss: 7.861, Perplexity: 2594.305
train[4220] loss: 7.841, Perplexity: 2543.383
train[4230] loss: 8.255, Perplexity: 3845.899
train[4240] loss: 8.229, Perplexity: 3748.714
train[4250] loss: 8.517, Perplexity: 5001.439
train[4260] loss: 8.411, Perplexity: 4496.713
train[4270] loss: 8.337, Perplexity: 4174.925
train[4280] loss: 8.437, Perplexity: 4613.192
train[4290] loss: 7.245, Perplexity: 1401.531
train[4300] loss: 7.384, Perplexity: 1610.586
train[4310] loss: 7.121, Perplexity: 1237.600
train[4320] loss: 7.548, Perplexity: 1896.521
train[4330] loss: 7.904, Perplexity: 2707.791
train[4340] loss: 7.917, Perplexity: 2742.740
train[4350] loss: 7.979, Perplexity: 2918.749
train[4360] loss: 8.192, Perplexity: 3610.247
train[4370] loss: 7.817, Perplexity: 2483.347
train[4380] loss: 7.963, Perplexity: 2871.601
train[4390] loss: 7.629, Perplexity: 2056.085
train[4400] loss: 6.893, Perplexity: 985.346
 test_epoch[68] val[4400] loss: 3.527, Perplexity: 34.028, blue score: 0.998
train[4410] loss: 6.876, Perplexity: 968.616
train[4420] loss: 7.087, Perplexity: 1195.879
train[4430] loss: 7.183, Perplexity: 1317.048
train[4440] loss: 7.230, Perplexity: 1379.734
train[4450] loss: 7.459, Perplexity: 1735.297
train[4460] loss: 7.697, Perplexity: 2201.253
train[4470] loss: 7.581, Perplexity: 1960.065
train[4480] loss: 7.956, Perplexity: 2853.079
train[4490] loss: 7.674, Perplexity: 2152.254
train[4500] loss: 7.035, Perplexity: 1135.210
train[4510] loss: 6.733, Perplexity: 839.338
train[4520] loss: 6.774, Perplexity: 874.802
train[4530] loss: 7.115, Perplexity: 1230.013
train[4540] loss: 6.891, Perplexity: 983.149
train[4550] loss: 6.954, Perplexity: 1047.175
train[4560] loss: 7.364, Perplexity: 1577.618
train[4570] loss: 7.239, Perplexity: 1392.282
train[4580] loss: 7.317, Perplexity: 1506.385
train[4590] loss: 7.712, Perplexity: 2235.667
train[4600] loss: 7.349, Perplexity: 1554.285
 test_epoch[68] val[4600] loss: 3.551, Perplexity: 34.845, blue score: 0.992
train[4610] loss: 6.375, Perplexity: 586.722
train[4620] loss: 6.409, Perplexity: 607.050
train[4630] loss: 6.654, Perplexity: 775.809
train[4640] loss: 6.744, Perplexity: 848.636
train[4650] loss: 6.905, Perplexity: 997.704
train[4660] loss: 6.859, Perplexity: 952.122
train[4670] loss: 6.851, Perplexity: 944.413
train[4680] loss: 7.232, Perplexity: 1382.528
train[4690] loss: 6.989, Perplexity: 1084.779
train[4700] loss: 7.277, Perplexity: 1447.041
train[4710] loss: 7.127, Perplexity: 1244.654
train[4720] loss: 5.932, Perplexity: 376.974
train[4730] loss: 6.221, Perplexity: 503.067
train[4740] loss: 6.283, Perplexity: 535.531
train[4750] loss: 6.347, Perplexity: 570.743
train[4760] loss: 6.512, Perplexity: 672.837
train[4770] loss: 6.889, Perplexity: 981.732
train[4780] loss: 6.875, Perplexity: 967.358
train[4790] loss: 7.215, Perplexity: 1360.068
train[4800] loss: 6.628, Perplexity: 755.609
 test_epoch[68] val[4800] loss: 3.663, Perplexity: 38.968, blue score: 0.998
train[4810] loss: 6.821, Perplexity: 916.453
train[4820] loss: 6.500, Perplexity: 665.095
train[4830] loss: 5.818, Perplexity: 336.440
train[4840] loss: 6.060, Perplexity: 428.463
train[4850] loss: 6.186, Perplexity: 485.709
train[4860] loss: 6.265, Perplexity: 525.919
train[4870] loss: 6.414, Perplexity: 610.556
train[4880] loss: 6.496, Perplexity: 662.529
train[4890] loss: 6.366, Perplexity: 581.533
train[4900] loss: 6.936, Perplexity: 1029.067
train[4910] loss: 6.883, Perplexity: 975.723
train[4920] loss: 6.786, Perplexity: 885.469
train[4930] loss: 6.115, Perplexity: 452.436
train[4940] loss: 5.774, Perplexity: 321.884
train[4950] loss: 5.740, Perplexity: 311.081
train[4960] loss: 6.029, Perplexity: 415.486
train[4970] loss: 6.233, Perplexity: 509.311
train[4980] loss: 6.292, Perplexity: 540.460
train[4990] loss: 6.120, Perplexity: 454.977
train[5000] loss: 6.318, Perplexity: 554.432
 test_epoch[68] val[5000] loss: 3.857, Perplexity: 47.308, blue score: 0.991

